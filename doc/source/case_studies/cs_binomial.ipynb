{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "title: \"Case Study: Binomial regression model\"\n",
    "author: Florence Bockting\n",
    "date: \"{{ datetime.now().strftime('%Y-%m-%d') }}\"\n",
    "output:\n",
    "    general:\n",
    "        input: true\n",
    "    html:\n",
    "        toc: true\n",
    "        code_folding: hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pretty_jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flobo\\anaconda3\\envs\\make-my-prior\\Lib\\site-packages\\bayesflow\\trainers.py:27: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import patsy as pa\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "from MakeMyPrior.elicitation_wrapper import expert_model\n",
    "from MakeMyPrior.training import trainer\n",
    "from MakeMyPrior.helper_functions import group_obs, Exponential_unconstrained, Normal_unconstrained\n",
    "from MakeMyPrior.user_config import target_config, target_input\n",
    "from MakeMyPrior.helper_functions import print_restab, plot_priors_hyp, plot_expert_preds, group_stats, plot_priors_flow\n",
    "import MakeMyPrior.combine_losses \n",
    "\n",
    "from tabulate import tabulate\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=4)\n",
    "\n",
    "tf.random.set_seed(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Background: Case Study\n",
    "We utilize a Binomial response distribution with a logit-link function for the probability parameter. As accompanying example, we use the Haberman’s survival dataset from the UCI machine learning repository. The dataset contains cases from a study that was\n",
    "conducted between 1958 and 1970 at the University of Chicago’s Billings Hospital on the survival of patients who had undergone surgery for breast cancer. In the following, we use the detected number of axillary lymph nodes that contain cancer (i.e., (positive) axillary nodes) as numerical predictor $X$ which consists in total of 31 observations ranging between 0 and 59 axillary nodes. The dependent variable $y$ is the number of patients who died within five years out of $T=100$ trials for each observation $i = 1, \\ldots ,N$. We consider a simple Binomial regression model with one continuous predictor.\n",
    "## Data generating model\n",
    "\\begin{align*}\n",
    "    y_i &\\sim \\text{Binomial}(T, \\theta_i)\\\\\n",
    "    \\text{logit}(\\theta_i) &= \\beta_0 + \\beta_1x_i\\\\\n",
    "    \\beta_k &\\sim \\text{Normal}(\\mu_k, \\sigma_k) \\quad \\text{for }k=0,1\\\\\n",
    "\\end{align*}\n",
    "The probability parameter $\\theta_i$ is predicted by a continuous predictor $x$ with an intercept $\\beta_0$ and slope $\\beta_1$. We assume normal priors for the regression coefficients, with mean $\\mu_k$ and standard deviation $\\sigma_k$ for $k=0,1$. Through the logit-link function, the probability $\\theta_i$ is mapped to the scale of the linear predictor. The objective is to learn the hyperparameters $\\lambda_k=(\\mu_k, \\sigma_k)$ based on expert knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology: Workflow\n",
    "[//]: # (-.- .tabset)\n",
    " \n",
    "+ General procedure:\n",
    "    + Draw samples from prior distribution(s) of model parameters \n",
    "    + Generate prior predictions according to the data generating model\n",
    "    + Compute the pre-defined target quantities\n",
    "    + Compute the elicited statistics of the target quantities\n",
    "    + Measure the discrepancy between the model-implied and the expert elicited statistics\n",
    "    + Update the weights of the transformation function of the normalizing flow  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method: Hyperparameter Learning\n",
    "[//]: # (-.- .tabset)\n",
    "\n",
    "### Method specification\n",
    "+ PriorSamples($\\lambda = (\\mu_k, \\sigma_k)$):\n",
    "\\begin{align*}\n",
    "\\{\\beta_k\\}_s &\\sim \\text{Normal}(\\mu_k, \\exp\\{\\sigma_k\\})\\\\\n",
    "\\end{align*}\n",
    "+ Generator($\\beta_k$):\n",
    "\\begin{align*}\n",
    "\\{\\text{logit}(\\theta_i)\\}_s &= \\{\\beta_k\\}_s \\times X_i\\\\\n",
    "\\{y_i\\}_s &\\sim \\text{Binomial}(T, \\{\\theta_i\\}_s)\\\\\n",
    "\\end{align*}\n",
    "+ Targets($\\{y_i\\}_s$):\n",
    "\\begin{align*}\n",
    "\\{y_j\\}_s &= \\{y_j\\}_s \\quad \\text{ for } j = 0, 5, 10, ,\\ldots,30\\\\\n",
    "\\{R^2\\}_s &= \\frac{Var(\\{\\theta_i\\}_s)}{Var(\\{y_i\\}_s)} \n",
    "\\end{align*}\n",
    "+ Elicits($\\{y_{j}\\}_s, \\{R^2\\}_s$)\n",
    "\\begin{align*}\n",
    "\\text{Quantile-based:} \\quad &Q_p^{j} = Q_{p}^{j} \\{y_j\\}_s \\quad \\text{ for } j = 0, 5, 10, ,\\ldots,30, p = 0.1, \\ldots, 0.9\\\\\n",
    "\\text{Histogram-based:} \\quad &\\{R^2\\}_s =\\{R^2\\}_s \n",
    "\\end{align*}\n",
    "\n",
    "### User specification\n",
    "\n",
    "#### Setting hyperparameter for the learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: true, output_error: false, input_fold: show }\n",
    "\n",
    "# setting of hyperparameter of learning algorithm\n",
    "user_config = dict(                    \n",
    "        B = 2**8,                          \n",
    "        rep = 300,                         \n",
    "        epochs = 300,                      \n",
    "        view_ep = 30,\n",
    "        lr_decay = True,\n",
    "        lr0 = 0.01, \n",
    "        lr_min = 0.0001, \n",
    "        loss_dimensions = \"m,n:B\",   \n",
    "        loss_discrepancy = \"energy\", \n",
    "        loss_scaling = \"unscaled\",         \n",
    "        method = \"hyperparameter_learning\"  \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor: shape=(7,), dtype=float32, numpy=\n",
      "array([0.       , 0.4075872, 0.8151744, 1.2227615, 1.6303488, 2.037936 ,\n",
      "       4.238907 ], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# construct design matrix\n",
    "d = pd.read_csv('C:/Users/flobo/hyp_learn_prior/tests/haberman_prep.csv')\n",
    "X = tf.constant(d[\"no_axillary_nodes\"], dtype=tf.float32)\n",
    "x_sd = tf.math.reduce_std(X)\n",
    "# scale predictor\n",
    "X_scaled = tf.constant(X, dtype=tf.float32)/x_sd\n",
    "# select only data points that were selected from expert\n",
    "dmatrix = tf.gather(X_scaled, [0, 5, 10, 15, 20, 25, 30]) \n",
    "\n",
    "pp.pprint(dmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the 'ideal agent'\n",
    "Define an 'arbitrary' ground truth for the hyperparameter values for method validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True hyperparameter values:\n",
      "{'mu': [-0.51, 0.26], 'sigma': [0.06, 0.04]}\n"
     ]
    }
   ],
   "source": [
    "# true hyperparameter values for ideal_expert\n",
    "true_values = dict()\n",
    "true_values[\"mu\"] = [-0.51, 0.26]\n",
    "true_values[\"sigma\"] = [0.06, 0.04]\n",
    "\n",
    "# model parameters\n",
    "parameters_dict = dict()\n",
    "for i in range(2):\n",
    "    parameters_dict[f\"beta_{i}\"] = {\n",
    "            \"family\":  Normal_unconstrained(),\n",
    "            \"true\": tfd.Normal(true_values[\"mu\"][i], true_values[\"sigma\"][i]),\n",
    "            \"initialization\": [tfd.Uniform(0.,1.), tfd.Normal(tf.math.log(0.1), 0.001)]\n",
    "            }\n",
    "\n",
    "print(\"True hyperparameter values:\")\n",
    "pp.pprint(true_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the data generating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: true, output_error: false, input_fold: show }\n",
    "\n",
    "# generative model\n",
    "class GenerativeModel(tf.Module):\n",
    "    def __call__(self, \n",
    "                 parameters, # obligatory: samples from prior distributions; tf.Tensor\n",
    "                 dmatrix,    # optional: design matrix; tf.Tensor\n",
    "                 total_count,       # optiona: total count for Binomial likelihood\n",
    "                 **kwargs    # obligatory: possibility for further keyword arguments is needed \n",
    "                 ):  \n",
    "\n",
    "        # linear predictor\n",
    "        theta = tf.expand_dims(parameters[:,:,0], -1) + tf.expand_dims(parameters[:,:,1], -1)*dmatrix\n",
    "      \n",
    "        # map linear predictor to theta\n",
    "        epred = tf.sigmoid(theta)\n",
    "        \n",
    "        # define likelihood\n",
    "        likelihood = tfd.Binomial(\n",
    "            total_count = total_count, \n",
    "            probs = epred[:,:,:,None]\n",
    "        )\n",
    "        \n",
    "        return dict(likelihood = likelihood,      # obligatory: likelihood; callable\n",
    "                    ypred = None,                # obligatory: prior predictive data\n",
    "                    epred = epred,                # obligatory: samples from linear predictor\n",
    "                    theta = theta\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the target quantities and the elicitation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'combine_loss': ['by-group'],\n",
      " 'custom_target_function': [None],\n",
      " 'elicitation': ['quantiles'],\n",
      " 'internal_loss': [None],\n",
      " 'quantiles_specs': [(10, 20, 30, 40, 50, 60, 70, 80, 90)],\n",
      " 'target': ['y_obs']}\n"
     ]
    }
   ],
   "source": [
    "# specify target quantity, elicitation technique and loss combination\n",
    "t1 = target_config(target=\"y_obs\", \n",
    "                   elicitation = \"quantiles\", \n",
    "                   combine_loss = \"by-group\", \n",
    "                   quantiles_specs = (10, 20, 30, 40, 50, 60, 70, 80, 90))\n",
    "\n",
    "\n",
    "target_info = target_input(t1)\n",
    "\n",
    "pp.pprint(target_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate from the \"ideal\" expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['y_obs_quant_0'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -.-|m { input: true, output_error: false, input_fold: show }\n",
    "\n",
    "expert_res_list, prior_pred_res = expert_model(1, user_config[\"rep\"],\n",
    "                                   parameters_dict, GenerativeModel, target_info,\n",
    "                                   method = \"ideal_expert\",\n",
    "                                   dmatrix = dmatrix,\n",
    "                                   total_count = 30)\n",
    "\n",
    "# elicited statistics from the (ideal) expert\n",
    "expert_res_list.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expert predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no.axillary.nodes': [0, 5, 10, 15, 20, 25, 30],\n",
      " 'no.pat.died': array([11.218554, 12.015126, 12.821019, 13.578913, 14.263377, 15.052743,\n",
      "       19.274298], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# number of patients who died within five years out of 𝑇 = 100 trials\n",
    "# for 0, 5, 10, 15, 20, 25, 30 axillary nodes\n",
    "pp.pprint(dict({\n",
    "    \"no.axillary.nodes\": [0, 5, 10, 15, 20, 25, 30],\n",
    "    \"no.pat.died\": tf.reduce_mean(tf.stack(expert_res_list[\"y_obs_quant_0\"], -1), (0,1)).numpy()\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m df_pd \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[0;32m     17\u001b[0m custom_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxes.spines.right\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxes.spines.top\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[1;32m---> 18\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241m.\u001b[39mset_theme(style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mticks\u001b[39m\u001b[38;5;124m\"\u001b[39m, rc\u001b[38;5;241m=\u001b[39mcustom_params)\n\u001b[0;32m     20\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m, layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstrained\u001b[39m\u001b[38;5;124m'\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m     21\u001b[0m [sns\u001b[38;5;241m.\u001b[39mscatterplot(y \u001b[38;5;241m=\u001b[39m j, x \u001b[38;5;241m=\u001b[39m q_exp[j][\u001b[38;5;241m0\u001b[39m,:], ax \u001b[38;5;241m=\u001b[39m axs, zorder \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m7\u001b[39m)]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "# plot expert predictions\n",
    "from statsmodels.graphics.boxplots import violinplot\n",
    "import polars as pl\n",
    "\n",
    "q_exp = expert_res_list[\"y_obs_quant_0\"]\n",
    "\n",
    "df = pl.DataFrame( ) \n",
    "df = df.with_columns(\n",
    "    q = np.arange(0.1,1., 0.1)\n",
    ")\n",
    "for i,j in zip(range(7),[0, 5, 10, 15, 20, 25, 30]):\n",
    "    df = df.with_columns(pl.Series(f\"{j}\", q_exp[i][0,:].numpy())) \n",
    "df = df.melt(id_vars = \"q\", value_vars = [f\"{j}\" for i,j in zip(range(7), [0, 5, 10, 15, 20, 25, 30])], variable_name = \"no.nodes\")\n",
    "\n",
    "df_pd = df.to_pandas()\n",
    "\n",
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "sns.set_theme(style=\"ticks\", rc=custom_params)\n",
    "\n",
    "fig, axs = plt.subplots(1,1, layout='constrained', figsize=(6, 3))\n",
    "[sns.scatterplot(y = j, x = q_exp[j][0,:], ax = axs, zorder = 1) for j in range(7)]\n",
    "sns.boxplot(x = df_pd[\"value\"], y = df_pd[\"no.nodes\"], color=\".99\",  linewidth=.75, zorder = 0) \n",
    "axs.set_ylabel(\"number of nodes\")\n",
    "axs.set_xlabel(\"$Q_p^G$\")\n",
    "axs.set_title(\"Quantile-based elicitation\", loc = \"left\", pad = 10., fontdict = {'fontsize': 14}) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the prior distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = trainer(expert_res_list, user_config[\"B\"], user_config[\"rep\"],\n",
    "                   parameters_dict, user_config[\"method\"], GenerativeModel,\n",
    "                   target_info, user_config, loss_balancing = True, save_vals = [\"prior_preds\"],\n",
    "                   dmatrix = dmatrix, total_count = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "#### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "sns.set_theme(style=\"ticks\", rc=custom_params)\n",
    "_, axs = plt.subplots(1,1, constrained_layout = True, figsize = (6,3))\n",
    "plt.plot(np.arange(len(res_dict[\"loss_info\"])), res_dict[\"loss_info\"], lw = 3) \n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(r\"$L(\\lambda)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_vals = [j for i in zip(true_values[\"mu\"],true_values[\"sigma\"]) for j in i]\n",
    "\n",
    "tab, avg_res = print_restab(method = \"hyperparameter_learning\", \n",
    "                             num_vars = 4, \n",
    "                             res = res_dict[\"hyperparam_info\"], \n",
    "                             start = 5, \n",
    "                             precision = 3, \n",
    "                             true_values = true_vals)\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learned prior distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "sns.set_theme(style=\"ticks\", rc=custom_params)\n",
    "_, axs = plt.subplots(1,1, constrained_layout = True, figsize = (6,3))\n",
    "x = tf.range(-0.8,0.5,0.01)\n",
    "sns.lineplot(x=x,y=tfd.Normal(avg_res[0], avg_res[1]).prob(x), lw = 3, label = rf\"$\\beta_0 \\sim$ N({avg_res[0]:.2f}, {avg_res[1]:.2f})\")\n",
    "sns.lineplot(x=x,y=tfd.Normal(avg_res[2], avg_res[3]).prob(x), lw = 3, label = rf\"$\\beta_1 \\sim$ N({avg_res[2]:.2f}, {avg_res[3]:.2f})\")\n",
    "sns.lineplot(x=x,y=tfd.Normal(true_vals[0], true_vals[1]).prob(x), linestyle= \"dashed\", color = \"black\")\n",
    "sns.lineplot(x=x,y=tfd.Normal(true_vals[2], true_vals[3]).prob(x), linestyle = \"dashed\", color = \"black\")\n",
    "axs.set_xlabel(r\"$\\beta_0, \\beta_1 \\sim$ Normal($\\cdot$, $\\cdot$)\")\n",
    "axs.set_ylabel(\"density\") \n",
    "axs.legend(handlelength = 0.4, fontsize = \"medium\") \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
