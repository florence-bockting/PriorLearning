{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d282108a-0c8c-4f56-844f-4bdc5e204751",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'make_my_prior.target_quantities'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tfd \u001b[38;5;241m=\u001b[39m tfp\u001b[38;5;241m.\u001b[39mdistributions\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mMakeMyPrior\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01melicitation_wrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m expert_model\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mMakeMyPrior\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trainer\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mMakeMyPrior\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelper_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m group_obs, Exponential_unconstrained, Normal_unconstrained\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MakeMyPrior\\lib\\site-packages\\MakeMyPrior\\elicitation_wrapper.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfp\u001b[39;00m\n\u001b[0;32m      8\u001b[0m tfd \u001b[38;5;241m=\u001b[39m tfp\u001b[38;5;241m.\u001b[39mdistributions\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmake_my_prior\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtarget_quantities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TargetQuantities\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmake_my_prior\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01melicitation_techniques\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ElicitationTechnique\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmake_my_prior\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_helpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m combine_loss_components\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'make_my_prior.target_quantities'"
     ]
    }
   ],
   "source": [
    "import patsy as pa\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "from MakeMyPrior.elicitation_wrapper import expert_model\n",
    "from MakeMyPrior.training import trainer\n",
    "from MakeMyPrior.helper_functions import group_obs, Exponential_unconstrained, Normal_unconstrained\n",
    "from MakeMyPrior.user_config import target_config, target_input\n",
    "from MakeMyPrior.plot_helpers import plot_loss, plot_convergence, plot_priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9721e0-7d32-4ce2-9972-38c504f8c3b8",
   "metadata": {},
   "source": [
    "# Normal Model\n",
    "## User specification\n",
    "\n",
    "### General variables for the learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670376f5-5e7e-4f0a-8a58-84fad3a0fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_config = dict(                    \n",
    "    B = 2**8,                          \n",
    "    rep = 600,                         \n",
    "    epochs = 6,#00,                      \n",
    "    view_ep = 1,\n",
    "    lr_decay = False,\n",
    "    lr0 = 0.01, \n",
    "    lr_min = 0.01, \n",
    "    loss_dimensions = \"m,n:B\",          \n",
    "    loss_scaling = \"unscaled\",         \n",
    "    method = \"hyperparameter_learning\"  \n",
    "    )\n",
    "    \n",
    "user_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99423e7b-457c-450e-b69a-193f28b663a6",
   "metadata": {},
   "source": [
    "### Design matrix\n",
    "\n",
    "(Only the first 10 observations are presented.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8223b-3f12-4511-999e-25e088ca9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# design matrix\n",
    "X =  pa.dmatrix(\"a*b\", pa.balanced(a = 2, b = 3, repeat = 60))\n",
    "dmatrix = tf.cast(X, dtype = tf.float32)\n",
    "# contrast matrix\n",
    "cmatrix = dmatrix[0:dmatrix.shape[1], :]\n",
    "\n",
    "# show first 10 rows\n",
    "dmatrix[:10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d85d199-7a6e-417b-8553-e465a2951260",
   "metadata": {},
   "source": [
    "## Model building\n",
    "\n",
    "\\begin{align}\n",
    "    y_i &\\sim \\textrm{Normal}(\\theta_i, s) \\\\\n",
    "    \\theta_i &= \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\beta_4x_4 + \\beta_5x_5\\\\\n",
    "    \\beta_k &\\sim \\textrm{Normal}(\\mu_k, \\sigma_k) \\quad \\textrm{ for }k=0,\\ldots,5\\\\\n",
    "     s &\\sim \\textrm{Exponential}(\\nu)\\\\\n",
    "\\end{align}\n",
    "\n",
    "### Prior distributions\n",
    "\n",
    "Specify...\n",
    "\n",
    "-   prior distribution family for model parameters\n",
    "-   distributions for initializing hyperparameter values of parametric prior distributions\n",
    "-   prior distribution of ideal expert with hyperparameter values representing the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c610c5-ba5b-424e-83f4-93ca23d95042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true hyperparameter values for ideal_expert\n",
    "true_mu = [0.12, 0.15, -0.02, -0.03, -0.02, -0.04]\n",
    "true_sigma = [0.02, 0.02, 0.06, 0.06, 0.03, 0.03]\n",
    "true_nu = 9.\n",
    "\n",
    "# model parameters\n",
    "parameters_dict = dict()\n",
    "for i in range(6):\n",
    "    parameters_dict[f\"beta_{i}\"] = {\n",
    "            \"family\":  Normal_unconstrained(),\n",
    "            \"true\": tfd.Normal(true_mu[i], true_sigma[i]),\n",
    "            \"initialization\": [tfd.Normal(0.,0.1)]*2\n",
    "            }\n",
    "parameters_dict[\"sigma\"] = {\n",
    "        \"family\": Exponential_unconstrained(user_config[\"rep\"]),\n",
    "        \"true\": tfd.Exponential(true_nu),\n",
    "        \"initialization\": [tfd.Normal(0.,0.1)]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdbaa1b-df1f-49e6-9f6b-b68880323c8c",
   "metadata": {},
   "source": [
    "### Generative model\n",
    "\n",
    "Necessary Input-Output structure for continuous likelihood:\n",
    "\n",
    "-   **Input**: `parameters`\n",
    "-   **Output**: `likelihood`, `ypred`, `epred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6bd400-2009-48ab-a35b-99701e02fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generative model\n",
    "class GenerativeModel(tf.Module):\n",
    "    def __call__(self, \n",
    "                 parameters, # obligatory: samples from prior distributions; tf.Tensor\n",
    "                 dmatrix,    # optional: design matrix; tf.Tensor\n",
    "                 cmatrix,    # optional: contrast matrix; tf.Tensor\n",
    "                 **kwargs    # obligatory: possibility for further keyword arguments is needed \n",
    "                 ):  \n",
    "        \n",
    "        # compute linear predictor term\n",
    "        epred = parameters[:,:,0:6] @ tf.transpose(dmatrix)\n",
    "        \n",
    "        # define likelihood\n",
    "        likelihood = tfd.Normal(\n",
    "            loc = epred, \n",
    "            scale = tf.expand_dims(parameters[:,:,-1], -1))\n",
    "        \n",
    "        # sample prior predictive data\n",
    "        ypred = likelihood.sample()\n",
    "        \n",
    "        # compute custom target quantity (here: group-differences)\n",
    "        samples_grouped = group_obs(ypred, dmatrix, cmatrix)\n",
    "\n",
    "        # compute mean difference between groups\n",
    "        effect_list = []\n",
    "        diffs = [(0,3), (1,4), (2,5)]\n",
    "        for i in range(len(diffs)):\n",
    "            # compute group difference\n",
    "            diff = tf.math.subtract(\n",
    "                samples_grouped[:, :, :, diffs[i][0]],\n",
    "                samples_grouped[:, :, :, diffs[i][1]],\n",
    "            )\n",
    "            # average over individual obs within each group\n",
    "            diff_mean = tf.reduce_mean(diff, axis=2)\n",
    "            # collect all mean group differences\n",
    "            effect_list.append(diff_mean)\n",
    "\n",
    "        mean_effects = tf.stack(effect_list, axis=-1)\n",
    "        \n",
    "        return dict(likelihood = likelihood,     # obligatory: likelihood; callable\n",
    "                    ypred = ypred,               # obligatory: prior predictive data\n",
    "                    epred = epred,               # obligatory: samples from linear predictor\n",
    "                    mean_effects = mean_effects  # optional: custom target quantity\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7221db4-6ce3-4785-8c5d-8f6a14702587",
   "metadata": {},
   "source": [
    "## Loss components\n",
    "\n",
    "(target quantities, elicitation techniques, combining losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3acff5-81ff-40e8-a40c-ed475d2d92ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom function using the output from the generative model   \n",
    "def custom_r2(ypred, epred, **kwargs):\n",
    "    return tf.math.divide(tf.math.reduce_variance(epred, axis = -1), \n",
    "                          tf.math.reduce_variance(ypred, axis = -1))\n",
    "\n",
    "# specify target quantity, elicitation technique and loss combination\n",
    "t1 = target_config(target=\"R2\", \n",
    "                   elicitation=\"histogram\",\n",
    "                   combine_loss=\"all\",\n",
    "                   custom_target_function = custom_r2)\n",
    "t2 = target_config(target=\"group_means\", \n",
    "                   elicitation=\"quantiles\", \n",
    "                   combine_loss=\"by-group\", \n",
    "                   quantiles_specs = (10, 20, 30, 40, 50, 60, 70, 80, 90))\n",
    "t3 = target_config(target=\"mean_effects\", \n",
    "                   elicitation=\"quantiles\",\n",
    "                   combine_loss=\"by-group\",\n",
    "                   quantiles_specs = (10, 20, 30, 40, 50, 60, 70, 80, 90))\n",
    "\n",
    "target_info = target_input(t1, t2, t3)\n",
    "\n",
    "target_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c13953-66e5-4572-a452-d9e356fb7b34",
   "metadata": {},
   "source": [
    "### Number and Shape of loss components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a664b9e-285e-4862-b65e-1e7b58ff4aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideal expert\n",
    "expert_res_list, prior_pred_res = expert_model(1, user_config[\"rep\"],\n",
    "                               parameters_dict, GenerativeModel, target_info,\n",
    "                               method = \"ideal_expert\",\n",
    "                               dmatrix = dmatrix,\n",
    "                               cmatrix = cmatrix,\n",
    "                               dmatrix_fct = dmatrix)\n",
    "                               \n",
    "for i in range(len(expert_res_list)):\n",
    "  print(f\"loss component {i}, with shape = \", expert_res_list[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0c4d22-a5be-4a31-bebf-30c786126d75",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d08a50f-cb3f-441e-8d70-904bb8c0628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation model and training\n",
    "res_dict = trainer(expert_res_list, user_config[\"B\"], user_config[\"rep\"],\n",
    "                   parameters_dict, user_config[\"method\"], GenerativeModel,\n",
    "                   target_info, user_config, loss_balancing = True,\n",
    "                   dmatrix = dmatrix, cmatrix = cmatrix, dmatrix_fct = dmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43d77bb-1a85-4d18-87e5-b70d48009d45",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a1c886-3100-4269-bd63-effc2e4b000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(user_config, res_dict)\n",
    "\n",
    "plot_convergence(user_config, res_dict)\n",
    "\n",
    "plot_priors(res_dict, ideal_expert_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}